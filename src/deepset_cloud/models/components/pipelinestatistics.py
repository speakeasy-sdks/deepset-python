"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from deepset_cloud import utils
from typing import Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class PipelineStatistics:
    accuracy_of_answers: Optional[float] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('accuracy_of_answers'), 'exclude': lambda f: f is None }})
    r"""The accuracy of answers is calculated by dividing the total number of positive feedback by the total number of given feedback. Pipelines with feedback on multiple responses will be counted only once."""
    avg_feedback_per_query: Optional[float] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('avg_feedback_per_query'), 'exclude': lambda f: f is None }})
    r"""The average number of feedback messages per query."""
    correct_answers: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('correct_answers'), 'exclude': lambda f: f is None }})
    r"""The number of answers marked as correct."""
    experiments_created: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('experiments_created'), 'exclude': lambda f: f is None }})
    r"""The total number of experiments created for this pipeline."""
    indexed_files: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('indexed_files'), 'exclude': lambda f: f is None }})
    r"""The total number of files indexed for this pipeline."""
    latency: Optional[float] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('latency'), 'exclude': lambda f: f is None }})
    r"""The average latency of the queries in seconds."""
    manual_feedback_input: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('manual_feedback_input'), 'exclude': lambda f: f is None }})
    r"""The total number of manual feedback inputs provided by users for this pipeline."""
    total_documents: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('total_documents'), 'exclude': lambda f: f is None }})
    r"""The total number of documents created for this pipeline."""
    total_queries: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('total_queries'), 'exclude': lambda f: f is None }})
    r"""The total number of queries for this pipeline."""
    users_provided_feedback: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('users_provided_feedback'), 'exclude': lambda f: f is None }})
    r"""The total number of unique feedback messages the users provided for the results of this pipeline."""
    wrong_answers: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('wrong_answers'), 'exclude': lambda f: f is None }})
    r"""The number of answers marked as wrong."""
    wrong_answers_with_correct_document: Optional[int] = dataclasses.field(default=0, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('wrong_answers_with_correct_document'), 'exclude': lambda f: f is None }})
    r"""The number of answers marked as wrong by the user while the document was marked as correct."""
    

